"""
ç½‘è¯¾è‡ªåŠ¨ç­”é¢˜ AI æœåŠ¡ - ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç‰ˆæœ¬
æ”¯æŒç›´æ¥åŠ è½½æœ¬åœ°æ¨¡å‹ï¼Œæ— éœ€Ollama
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import uvicorn
import re
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="AIç­”é¢˜æœåŠ¡ - æœ¬åœ°æ¨¡å‹ç‰ˆ")

# æ·»åŠ CORSæ”¯æŒ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================
# é…ç½®åŒºåŸŸ - è¯·æ ¹æ®æ‚¨çš„å®é™…æƒ…å†µä¿®æ”¹
# ============================================================

# æ¨¡å‹è·¯å¾„é…ç½®
MODEL_CONFIG = {
    # é€‰é¡¹1: ä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼ˆæ¨èï¼‰
    "model_path": r"D:\models\Qwen2.5-7B-Instruct",  # ä¿®æ”¹ä¸ºæ‚¨çš„æ¨¡å‹è·¯å¾„
    
    # é€‰é¡¹2: ä»Hugging Faceä¸‹è½½ï¼ˆå¦‚æœæœ¬åœ°æ²¡æœ‰ï¼‰
    # "model_path": "Qwen/Qwen2.5-7B-Instruct",
    
    # å…¶ä»–å¯é€‰æ¨¡å‹ï¼š
    # "model_path": "Qwen/Qwen2.5-3B-Instruct",
    # "model_path": "Qwen/Qwen2.5-1.5B-Instruct",
    # "model_path": r"D:\models\qwen2.5-14b-instruct",
    
    # æ¨¡å‹åŠ è½½å‚æ•°
    "device_map": "auto",  # è‡ªåŠ¨åˆ†é…è®¾å¤‡ï¼ˆCPU/GPUï¼‰
    "torch_dtype": torch.float16,  # ä½¿ç”¨åŠç²¾åº¦ï¼ˆå¦‚æœGPUä¸æ”¯æŒï¼Œæ”¹ä¸ºtorch.float32ï¼‰
    
    # ç”Ÿæˆå‚æ•°
    "max_new_tokens": 256,
    "temperature": 0.3,
    "top_p": 0.9,
    "top_k": 40,
}

# ============================================================
# å…¨å±€æ¨¡å‹å˜é‡
# ============================================================
tokenizer = None
model = None

class QuestionRequest(BaseModel):
    question: str
    options: Optional[List[str]] = []
    type: str  # 0:å•é€‰ 1:å¤šé€‰ 3:åˆ¤æ–­ 2:å¡«ç©º 4:ç®€ç­”
    questionData: Optional[str] = ""
    workType: Optional[str] = ""
    id: Optional[str] = ""
    key: Optional[str] = ""

class AnswerResponse(BaseModel):
    code: int
    data: dict
    msg: str

def load_model():
    """åŠ è½½æ¨¡å‹ï¼ˆå¯åŠ¨æ—¶è°ƒç”¨ä¸€æ¬¡ï¼‰"""
    global tokenizer, model
    
    try:
        logger.info(f"å¼€å§‹åŠ è½½æ¨¡å‹: {MODEL_CONFIG['model_path']}")
        logger.info("è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_CONFIG['model_path'],
            trust_remote_code=True
        )
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_CONFIG['model_path'],
            device_map=MODEL_CONFIG['device_map'],
            torch_dtype=MODEL_CONFIG['torch_dtype'],
            trust_remote_code=True
        )
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()
        
        logger.info("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        logger.info(f"è®¾å¤‡: {next(model.parameters()).device}")
        logger.info(f"ç²¾åº¦: {next(model.parameters()).dtype}")
        
        return True
        
    except Exception as e:
        logger.error(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        logger.error("è¯·æ£€æŸ¥:")
        logger.error("1. æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
        logger.error("2. æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´")
        logger.error("3. Pythonå†…å­˜æ˜¯å¦è¶³å¤Ÿ")
        return False

def generate_answer(prompt: str) -> str:
    """ä½¿ç”¨æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ"""
    try:
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç­”é¢˜åŠ©æ‰‹ï¼Œè¯·ç›´æ¥ç»™å‡ºå‡†ç¡®ç­”æ¡ˆï¼Œä¸è¦æ·»åŠ é¢å¤–è§£é‡Šã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            enable_thinking=False,
            add_generation_prompt=True
        )
        
        # ç¼–ç 
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=MODEL_CONFIG['max_new_tokens'],
                temperature=MODEL_CONFIG['temperature'],
                top_p=MODEL_CONFIG['top_p'],
                top_k=MODEL_CONFIG['top_k'],
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        # è§£ç ï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response.strip()
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆé”™è¯¯: {e}")
        return ""

def parse_answer(answer_text: str, q_type: str, options: List[str]) -> List[str]:
    """è§£æAIè¿”å›çš„ç­”æ¡ˆ"""
    
    if q_type == "0":  # å•é€‰é¢˜
        match = re.search(r'[A-Z]', answer_text)
        if match:
            letter = match.group(0)
            idx = ord(letter) - 65
            if 0 <= idx < len(options):
                return [options[idx]]
        return []
    
    elif q_type == "1":  # å¤šé€‰é¢˜
        letters = re.findall(r'[A-Z]', answer_text)
        answers = []
        for letter in letters:
            idx = ord(letter) - 65
            if 0 <= idx < len(options):
                answers.append(options[idx])
        return answers if answers else []
    
    elif q_type == "3":  # åˆ¤æ–­é¢˜
        lower = answer_text.lower()
        if "æ­£ç¡®" in answer_text or "true" in lower or "å¯¹" in answer_text or "yes" in lower:
            return ["æ­£ç¡®"]
        elif "é”™è¯¯" in answer_text or "false" in lower or "é”™" in answer_text or "no" in lower:
            return ["é”™è¯¯"]
        return []
    
    else:  # å¡«ç©ºé¢˜ã€ç®€ç­”é¢˜ç­‰
        answer_text = answer_text.replace("ç­”æ¡ˆï¼š", "").replace("ç­”æ¡ˆ:", "").strip()
        # ç§»é™¤å¸¸è§çš„å‰ç¼€
        answer_text = re.sub(r'^(ç­”æ¡ˆ?[:ï¼š]?\s*)', '', answer_text)
        return [answer_text] if answer_text else []

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    logger.info("=" * 60)
    logger.info("AIç­”é¢˜æœåŠ¡å¯åŠ¨ä¸­...")
    logger.info("=" * 60)
    
    success = load_model()
    
    if not success:
        logger.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼ŒæœåŠ¡å°†ä»¥é”™è¯¯æ¨¡å¼è¿è¡Œ")
        logger.error("è¯·ä¿®æ”¹ MODEL_CONFIG ä¸­çš„ model_path")
    else:
        logger.info("=" * 60)
        logger.info("âœ“ æœåŠ¡å¯åŠ¨æˆåŠŸï¼")
        logger.info(f"APIåœ°å€: http://localhost:5000")
        logger.info(f"APIæ–‡æ¡£: http://localhost:5000/docs")
        logger.info("=" * 60)

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "AIç­”é¢˜æœåŠ¡è¿è¡Œä¸­",
        "status": "ok" if model is not None else "error",
        "model_loaded": model is not None,
        "model_path": MODEL_CONFIG['model_path']
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    if model is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªåŠ è½½")
    return {
        "status": "healthy",
        "model": MODEL_CONFIG['model_path'],
        "device": str(next(model.parameters()).device)
    }
    
@app.post("/search")
async def search_answer(request: QuestionRequest):
    """ç­”é¢˜æ¥å£ï¼ˆå…¼å®¹è„šæœ¬æ ¼å¼ï¼‰ - å·²ä¿®å¤ç¹ä½“ä¸­æ–‡é¢˜å‹è¯†åˆ«é—®é¢˜"""
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½
    if model is None or tokenizer is None:
        return AnswerResponse(
            code=10003,
            data={"answer": [], "num": "", "usenum": ""},
            msg="æ¨¡å‹æœªåŠ è½½ï¼Œè¯·æ£€æŸ¥é…ç½®"
        )
    
    try:
        # ============================================================
        # 1. é¢˜å‹æ ‡å‡†åŒ–æ˜ å°„ (æ ¸å¿ƒä¿®å¤éƒ¨åˆ†)
        # ============================================================
        # å°†å„ç§è¾“å…¥(ç¹ä½“/ç®€ä½“/æ•°å­—)ç»Ÿä¸€æ˜ å°„ä¸ºå†…éƒ¨ä»£ç 
        NORMALIZE_TYPE_MAP = {
            # å•é€‰é¢˜ -> "0"
            "0": "0", "å•é€‰é¢˜": "0", "å–®é¸é¡Œ": "0", "å•é€‰": "0", "å–®é¸": "0", "Single": "0",
            # å¤šé€‰é¢˜ -> "1"
            "1": "1", "å¤šé€‰é¢˜": "1", "å¤šé¸é¡Œ": "1", "å¤šé€‰": "1", "å¤šé¸": "1", "Multiple": "1",
            # å¡«ç©ºé¢˜ -> "2"
            "2": "2", "å¡«ç©ºé¢˜": "2", "å¡«ç©ºé¡Œ": "2", "å¡«ç©º": "2",
            # åˆ¤æ–­é¢˜ -> "3"
            "3": "3", "åˆ¤æ–­é¢˜": "3", "åˆ¤æ–·é¡Œ": "3", "åˆ¤æ–­": "3", "åˆ¤æ–·": "3", "TrueOrFalse": "3",
            # ç®€ç­”é¢˜ -> "4"
            "4": "4", "ç®€ç­”é¢˜": "4", "ç°¡ç­”é¡Œ": "4", "ç®€ç­”": "4", "ç°¡ç­”": "4",
            # åè¯è§£é‡Š -> "5"
            "5": "5", "åè¯è§£é‡Š": "5", "åè©è§£é‡‹": "5",
            # è®ºè¿°é¢˜ -> "6"
            "6": "6", "è®ºè¿°é¢˜": "6", "è«–è¿°é¡Œ": "6",
            # è®¡ç®—é¢˜ -> "7"
            "7": "7", "è®¡ç®—é¢˜": "7", "è¨ˆç®—é¡Œ": "7"
        }

        # è·å–æ ‡å‡†åŒ–çš„é¢˜å‹ä»£ç  (å¦‚æœæ²¡æ‰¾åˆ°ï¼Œé»˜è®¤å½“åšç®€ç­”é¢˜"4"å¤„ç†ï¼Œé¿å…æŠ¥é”™)
        # strip() å»é™¤å¯èƒ½å­˜åœ¨çš„ç©ºæ ¼
        raw_type = str(request.type).strip()
        standard_type = NORMALIZE_TYPE_MAP.get(raw_type, "4")

        # ç”¨äºæ˜¾ç¤ºç»™ LLM çš„ä¸­æ–‡åç§°
        type_name_map = {
            "0": "å•é€‰é¢˜",
            "1": "å¤šé€‰é¢˜",
            "3": "åˆ¤æ–­é¢˜",
            "2": "å¡«ç©ºé¢˜",
            "4": "ç®€ç­”é¢˜",
            "5": "åè¯è§£é‡Š",
            "6": "è®ºè¿°é¢˜",
            "7": "è®¡ç®—é¢˜"
        }
        
        q_type_name = type_name_map.get(standard_type, "ç®€ç­”é¢˜")
        
        # ============================================================
        # 2. æ„å»º Prompt
        # ============================================================
        prompt = f"""è¯·ä»”ç»†åˆ†æä»¥ä¸‹é¢˜ç›®å¹¶ç»™å‡ºæ­£ç¡®ç­”æ¡ˆã€‚

é¢˜ç›®ç±»å‹: {q_type_name}
é¢˜ç›®: {request.question}
"""
        
        if request.options:
            prompt += "\né€‰é¡¹:\n"
            for i, opt in enumerate(request.options):
                # æ¸…ç†é€‰é¡¹å†…å®¹ï¼Œé˜²æ­¢æ ¼å¼æ··ä¹±
                clean_opt = str(opt).strip()
                prompt += f"{chr(65+i)}. {clean_opt}\n"
        
        # æ ¹æ®æ ‡å‡†åŒ–åçš„é¢˜å‹æ·»åŠ ç‰¹å®šæŒ‡ä»¤
        prompt += "\nè¯·ç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šï¼š\n"
        if standard_type == "0":
            prompt += "- å•é€‰é¢˜ï¼šåªå›ç­”ä¸€ä¸ªé€‰é¡¹å­—æ¯ï¼Œå¦‚ A"
        elif standard_type == "1":
            prompt += "- å¤šé€‰é¢˜ï¼šå›ç­”æ‰€æœ‰æ­£ç¡®é€‰é¡¹å­—æ¯ï¼Œç”¨é€—å·åˆ†éš”ï¼Œå¦‚ A,C,D"
        elif standard_type == "3":
            prompt += "- åˆ¤æ–­é¢˜ï¼šåªå›ç­”'æ­£ç¡®'æˆ–'é”™è¯¯'"
        else:
            prompt += "- å¡«ç©ºæˆ–ç®€ç­”ï¼šç›´æ¥ç»™å‡ºç­”æ¡ˆå†…å®¹"

        prompt += "\n\nç­”æ¡ˆï¼š"
        
        logger.info(f"[ç­”é¢˜] åŸå§‹ç±»å‹: {raw_type} -> è¯†åˆ«ä¸º: {q_type_name} ({standard_type})")
        logger.info(f"[ç­”é¢˜] é¢˜ç›®: {request.question[:50]}...")
        
        # ============================================================
        # 3. ç”Ÿæˆä¸è§£æ
        # ============================================================
        ai_answer = generate_answer(prompt)
        logger.info(f"[ç­”é¢˜] AIå›ç­”: {ai_answer}")
        
        # ä½¿ç”¨æ ‡å‡†åŒ–åçš„ standard_type è¿›è¡Œè§£æï¼Œç¡®ä¿è§£æé€»è¾‘æ­£ç¡®
        answers = parse_answer(ai_answer, standard_type, request.options)
        
        if answers:
            logger.info(f"[ç­”é¢˜] è§£æç»“æœ: {answers}")
            return AnswerResponse(
                code=200,
                data={
                    "answer": answers,
                    "num": "Local AI",
                    "usenum": "æœ¬åœ°æ¨¡å‹"
                },
                msg="ç­”é¢˜æˆåŠŸ"
            )
        else:
            logger.warning("[ç­”é¢˜] æ— æ³•è§£æç­”æ¡ˆ")
            # å³ä½¿è§£æå¤±è´¥ï¼Œå¯¹äºéé€‰æ‹©é¢˜ä¹Ÿå°è¯•ç›´æ¥è¿”å›AIçš„åŸå§‹å›ç­”
            if standard_type not in ["0", "1", "3"]:
                 return AnswerResponse(
                    code=200,
                    data={
                        "answer": [ai_answer],
                        "num": "Local AI",
                        "usenum": "æœ¬åœ°æ¨¡å‹"
                    },
                    msg="ç­”é¢˜æˆåŠŸ(æœªæ ¼å¼åŒ–)"
                )

            return AnswerResponse(
                code=10003,
                data={"answer": [], "num": "", "usenum": ""},
                msg="AIæ— æ³•è§£ç­”æ­¤é¢˜"
            )
    
    except Exception as e:
        logger.error(f"[ç­”é¢˜] é”™è¯¯: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return AnswerResponse(
            code=10003,
            data={"answer": [], "num": "", "usenum": ""},
            msg=f"æœåŠ¡é”™è¯¯: {str(e)}"
        )
@app.post("/test")
async def test_generate(prompt: str = "ä½ å¥½"):
    """æµ‹è¯•ç”ŸæˆåŠŸèƒ½"""
    if model is None:
        return {"error": "æ¨¡å‹æœªåŠ è½½"}
    
    response = generate_answer(prompt)
    return {
        "prompt": prompt,
        "response": response
    }

if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("ğŸš€ AIç­”é¢˜æœåŠ¡ - æœ¬åœ°æ¨¡å‹ç‰ˆ")
    print("=" * 60)
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {MODEL_CONFIG['model_path']}")
    print(f"ğŸ”§ è®¾å¤‡æ¨¡å¼: {MODEL_CONFIG['device_map']}")
    print(f"ğŸ’¾ æ•°æ®ç±»å‹: {MODEL_CONFIG['torch_dtype']}")
    print("=" * 60)
    print("â³ æ­£åœ¨å¯åŠ¨æœåŠ¡...")
    print("=" * 60)
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=5000,
        log_level="info"
    )
